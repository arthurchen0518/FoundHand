{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e45170a",
   "metadata": {},
   "source": [
    "# Load the diffusion model, SAM, Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19a27ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load diffusion model...\n",
      "[] ['projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias', 'projector.4.weight', 'projector.4.bias']\n",
      "Mediapipe hand detector and SAM ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750404705.115486 3524965 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1750404705.144195 3702037 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.129.03), renderer: NVIDIA RTX A6000/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1750404705.194073 3701596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1750404705.205530 3701630 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os.path as osp\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from models import vqvae \n",
    "from models import vit\n",
    "from diffusion import create_diffusion\n",
    "from utils.utils import (\n",
    "    scale_keypoint,  \n",
    "    keypoint_heatmap, \n",
    "    check_keypoints_validity)\n",
    "from utils.segment_hoi import init_sam, show_mask\n",
    "import pickle\n",
    "        \n",
    "\n",
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix) :]\n",
    "    return text\n",
    "\n",
    "\n",
    "def unnormalize(x):\n",
    "    return (((x + 1) / 2) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def visualize_hand(ax, all_joints, img):\n",
    "# Define the connections between joints for drawing lines and their corresponding colors\n",
    "    connections = [\n",
    "        ((0, 1), 'red'), ((1, 2), 'green'), ((2, 3), 'blue'), ((3, 4), 'purple'),\n",
    "        ((0, 5), 'orange'), ((5, 6), 'pink'), ((6, 7), 'brown'), ((7, 8), 'cyan'),\n",
    "        ((0, 9), 'yellow'), ((9, 10), 'magenta'), ((10, 11), 'lime'), ((11, 12), 'indigo'),\n",
    "        ((0, 13), 'olive'), ((13, 14), 'teal'), ((14, 15), 'navy'), ((15, 16), 'gray'),\n",
    "        ((0, 17), 'lavender'), ((17, 18), 'silver'), ((18, 19), 'maroon'), ((19, 20), 'fuchsia')\n",
    "    ]\n",
    "    H, W, C = img.shape\n",
    "    \n",
    "    # Plot joints as points\n",
    "    ax.imshow(img)\n",
    "    for start_i in [0, 21]: \n",
    "        joints = all_joints[start_i: start_i+21]\n",
    "        for connection, color in connections:\n",
    "            joint1 = joints[connection[0]]\n",
    "            joint2 = joints[connection[1]]\n",
    "            ax.plot([joint1[0], joint2[0]], [joint1[1], joint2[1]], color=color)\n",
    "\n",
    "    ax.set_xlim([0, W])\n",
    "    ax.set_ylim([0, H])\n",
    "    ax.grid(False)\n",
    "    ax.set_axis_off()\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class HandDiffOpts:\n",
    "    run_name: str = 'ViT_256_handmask_heatmap_nvs_b25_lr1e-5'\n",
    "    sd_path: str = '/users/kchen157/scratch/weights/SD/sd-v1-4.ckpt'\n",
    "    log_dir: str = '/users/kchen157/scratch/log'\n",
    "    data_root: str = '/users/kchen157/data/users/kchen157/dataset/handdiff'\n",
    "    image_size: tuple = (256, 256)\n",
    "    latent_size: tuple = (32, 32)\n",
    "    latent_dim: int = 4\n",
    "    mask_bg: bool = False\n",
    "    kpts_form: str = 'heatmap'\n",
    "    n_keypoints: int = 42\n",
    "    n_mask: int = 1\n",
    "    noise_steps: int = 1000\n",
    "    test_sampling_steps: int = 250\n",
    "    ddim_steps: int = 100\n",
    "    ddim_discretize: str = \"uniform\"\n",
    "    ddim_eta: float = 0.\n",
    "    beta_start: float = 8.5e-4\n",
    "    beta_end: float = 0.012\n",
    "    latent_scaling_factor: float = 0.18215\n",
    "    cfg_pose: float = 5.\n",
    "    cfg_appearance: float = 3.5\n",
    "    batch_size: int = 25\n",
    "    lr: float = 1e-5\n",
    "    max_epochs: int = 500\n",
    "    log_every_n_steps: int = 100\n",
    "    limit_val_batches: int = 1\n",
    "    n_gpu: int = 8\n",
    "    num_nodes: int = 1\n",
    "    precision: str = '16-mixed'\n",
    "    profiler: str = 'simple'\n",
    "    swa_epoch_start: int = 10\n",
    "    swa_lrs: float = 1e-3\n",
    "    num_workers: int = 10\n",
    "    n_val_samples: int = 4\n",
    "        \n",
    "\n",
    "opts = HandDiffOpts()\n",
    "model_weights_dir = '../weights'\n",
    "model_path = osp.join(model_weights_dir, 'DINO_EMA_11M_b50_lr1e-5_epoch6_step320k.ckpt')\n",
    "vae_path = osp.join(model_weights_dir, 'vae-ft-mse-840000-ema-pruned.ckpt')\n",
    "sam_path = osp.join(model_weights_dir, 'sam_vit_h_4b8939.pth')\n",
    "\n",
    "print('Load diffusion model...')\n",
    "diffusion = create_diffusion(str(opts.test_sampling_steps))\n",
    "model = vit.DiT_XL_2(\n",
    "    input_size=opts.latent_size[0],\n",
    "    latent_dim=opts.latent_dim,\n",
    "    in_channels=opts.latent_dim+opts.n_keypoints+opts.n_mask,\n",
    "    learn_sigma=True,\n",
    ").cuda()\n",
    "\n",
    "# ckpt_state_dict = torch.load(model_path)['model_state_dict']\n",
    "ckpt_state_dict = torch.load(model_path)['ema_state_dict']\n",
    "missing_keys, extra_keys = model.load_state_dict(ckpt_state_dict, strict=False)\n",
    "model.eval()\n",
    "print(missing_keys, extra_keys)\n",
    "assert len(missing_keys) == 0\n",
    "\n",
    "\n",
    "vae_state_dict = torch.load(vae_path)['state_dict']\n",
    "autoencoder = vqvae.create_model(3, 3, opts.latent_dim).eval().requires_grad_(False).cuda()\n",
    "missing_keys, extra_keys = autoencoder.load_state_dict(vae_state_dict, strict=False)\n",
    "autoencoder.eval()\n",
    "assert len(missing_keys) == 0\n",
    "\n",
    "\n",
    "print('Mediapipe hand detector and SAM ready...')\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,  # Use False if image is part of a video stream\n",
    "    max_num_hands=2,         # Maximum number of hands to detect\n",
    "    min_detection_confidence=0.1)\n",
    "sam_predictor = init_sam(ckpt_path=sam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def draw_keypoint_trajectories(image_path, keypoints_list, image_size=(256, 256)):\n",
    "    # Create a blank image to draw on\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Number of keypoints (based on the first time step)\n",
    "    num_keypoints = 42\n",
    "    \n",
    "    # Use a color map to get different colors for each keypoint\n",
    "    cmap = plt.get_cmap('hsv')\n",
    "    colors = [cmap(i / num_keypoints) for i in range(num_keypoints)]\n",
    "    \n",
    "    # Iterate over keypoints\n",
    "    for kp_idx in range(num_keypoints):\n",
    "        # Iterate over time steps and draw lines connecting consecutive positions\n",
    "        for t in range(1, len(keypoints_list)):\n",
    "            pt1 = tuple(keypoints_list[t-1][kp_idx].astype(int))  # Previous position\n",
    "            pt2 = tuple(keypoints_list[t][kp_idx].astype(int))    # Current position\n",
    "            # Convert matplotlib color to BGR format (used in OpenCV)\n",
    "            color = tuple(int(255 * c) for c in colors[kp_idx][:3][::-1])  # Convert to BGR\n",
    "            # Draw line between consecutive points\n",
    "            cv2.line(image, pt1, pt2, color, 1)  # Line thickness of 2\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "data_root = '../test_data/iphone_video'\n",
    "idx = 'IMG_1173'\n",
    "start_frame = 6\n",
    "max_frames = 100\n",
    "image_file = osp.join(data_root, idx, f'{start_frame:04d}.jpg')\n",
    "path_file = osp.join(data_root, f'{idx}.pkl')\n",
    "\n",
    "image_idx = idx\n",
    "path_idx = idx\n",
    "right_hand_only = False\n",
    "with open(path_file, 'rb') as f: \n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(len(data[start_frame:start_frame+max_frames]))\n",
    "image_with_trajectories = draw_keypoint_trajectories(image_file, data[start_frame:start_frame+max_frames])\n",
    "\n",
    "# Display the resulting image using matplotlib\n",
    "plt.imshow(cv2.cvtColor(image_with_trajectories, cv2.COLOR_BGR2RGB))\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c388f",
   "metadata": {},
   "source": [
    "# Get sequence poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_file, 'rb') as f:\n",
    "    keypts_sequence = pickle.load(f)\n",
    "    \n",
    "target_conds = []\n",
    "for keypts in keypts_sequence[start_frame:start_frame+max_frames]:\n",
    "    kpts_valid = check_keypoints_validity(keypts, opts.image_size)\n",
    "    if right_hand_only:\n",
    "        kpts_valid[21:] *= 0 \n",
    "    target_heatmaps = torch.tensor(keypoint_heatmap(\n",
    "        scale_keypoint(keypts, opts.image_size, opts.latent_size), \n",
    "        opts.latent_size, var=1.) * kpts_valid[:, None, None], dtype=torch.float, device='cuda')[None, ...]\n",
    "    target_cond = torch.cat([\n",
    "        target_heatmaps, \n",
    "        torch.zeros((1, 1, opts.latent_size[0], opts.latent_size[1])).to(target_heatmaps)], 1)\n",
    "    target_conds.append(target_cond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a50b1",
   "metadata": {},
   "source": [
    "# Load the reference image and get annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ref_cond(img, keypts, hand_mask, device='cuda', target_size=(256, 256), latent_size=(32, 32)):\n",
    "    image_transform=Compose([\n",
    "        ToTensor(),\n",
    "        Resize(target_size),\n",
    "        Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], \n",
    "            std=[0.5, 0.5, 0.5], inplace=True),\n",
    "    ])\n",
    "    image = image_transform(img).to(device)\n",
    "    kpts_valid = check_keypoints_validity(keypts, target_size)\n",
    "    heatmaps = torch.tensor(keypoint_heatmap(\n",
    "        scale_keypoint(keypts, target_size, latent_size), \n",
    "        latent_size, var=1.) * kpts_valid[:, None, None], dtype=torch.float, device=device)[None, ...]\n",
    "    mask = torch.tensor(\n",
    "        cv2.resize(hand_mask.astype(int), dsize=latent_size, interpolation=cv2.INTER_NEAREST), \n",
    "        dtype=torch.float, device=device).unsqueeze(0)[None, ...]\n",
    "    return image[None, ...], heatmaps, mask\n",
    "\n",
    "ref_conds = []\n",
    "bootstrap_frames = [start_frame]\n",
    "for k, frame in enumerate(bootstrap_frames):\n",
    "    image_file = osp.join(data_root, idx, f'{frame:04d}.jpg')\n",
    "    flip_image = False\n",
    "    img = io.imread(image_file)\n",
    "    print(img.shape)\n",
    "    if flip_image:\n",
    "        img = np.fliplr(img)\n",
    "    img = cv2.resize(img, opts.image_size, interpolation=cv2.INTER_AREA)\n",
    "    keypts = keypts_sequence[frame]\n",
    "\n",
    "    sam_predictor.set_image(img)\n",
    "    l = keypts[:21].shape[0]\n",
    "    if keypts[0].sum() != 0 and keypts[21].sum() != 0:\n",
    "        input_point = np.array([keypts[0], keypts[21]])\n",
    "        input_label = np.array([1, 1])\n",
    "    elif keypts[0].sum() != 0:\n",
    "        input_point = np.array(keypts[:1])\n",
    "        input_label = np.array([1])\n",
    "    elif keypts[21].sum() != 0:\n",
    "        input_point = np.array(keypts[21:22])\n",
    "        input_label = np.array([1])\n",
    "    masks, _, _ = sam_predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    hand_mask = masks[0]\n",
    "    masked_img = img * hand_mask[..., None] + 255*(1 - hand_mask[..., None])\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(3*2, 3))\n",
    "    visualize_hand(axs[0], keypts, img)\n",
    "    visualize_hand(axs[1], keypts, masked_img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    image, heatmaps, mask = make_ref_cond(\n",
    "        img, keypts, hand_mask, device='cuda', target_size=opts.image_size, latent_size=opts.latent_size)\n",
    "    latent = opts.latent_scaling_factor * autoencoder.encode(image).sample()\n",
    "    if k == 0:\n",
    "        ref_image = img\n",
    "        ref_keypts = keypts\n",
    "        src_ref_cond = torch.cat([latent, heatmaps, mask], 1)\n",
    "    else:\n",
    "        ref_conds.append(torch.cat([latent, heatmaps, mask], 1))\n",
    "        \n",
    "print('ref_conds:', len(ref_conds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe21e0",
   "metadata": {},
   "source": [
    "# Sample from diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e70119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def frames_to_video(frames, output_path, fps=30, resize=None, rgb2bgr=False):\n",
    "    \"\"\"\n",
    "    Convert a list of frames (numpy arrays) to a video using OpenCV.\n",
    "\n",
    "    Args:\n",
    "    - frames (list or numpy.ndarray): List of numpy frames.\n",
    "    - output_path (str): Path where the video will be saved.\n",
    "    - fps (int): Frames per second.\n",
    "    - resize (tuple or None): Resize frames to (width, height) if not None.\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(\"No frames provided\")\n",
    "\n",
    "    height, width = frames[0].shape[:2]\n",
    "\n",
    "    # If a resize shape is provided, adjust width and height\n",
    "    if resize is not None:\n",
    "        width, height = resize\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in tqdm(frames):\n",
    "        if len(frames[0].shape) == 2:\n",
    "            frame = frame[..., None].repeat(3, axis=-1)\n",
    "        if resize is not None:\n",
    "            frame = cv2.resize(frame, (width, height))\n",
    "        if rgb2bgr:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "\n",
    "cfg_scale = 2.5\n",
    "last_N_frames = 1\n",
    "save_dir = './video_results'\n",
    "# novel view synthesis mode = off\n",
    "nvs = torch.zeros(1, dtype=torch.int, device='cuda')\n",
    "z = torch.randn((1, opts.latent_dim, opts.latent_size[0], opts.latent_size[1]), device='cuda')\n",
    "z = torch.cat([z, z], 0)\n",
    "\n",
    "temp_ref_conds = []\n",
    "video_frames = [] \n",
    "for k, target_cond in enumerate(target_conds): \n",
    "    print(f'{k}/{min(max_frames, len(target_conds))}')\n",
    "    model_kwargs = dict(\n",
    "        target_cond=torch.cat([target_cond, torch.zeros_like(target_cond)]), \n",
    "        ref_cond=torch.cat([src_ref_cond, torch.zeros_like(src_ref_cond)]), \n",
    "        nvs=torch.cat([nvs, 2 * torch.ones_like(nvs)]), \n",
    "        cfg_scale=cfg_scale)\n",
    "    \n",
    "    samples, _ = diffusion.p_sample_loop(\n",
    "        model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "        model_kwargs=model_kwargs, ref_conds=[src_ref_cond] + ref_conds + temp_ref_conds, progress=True, device='cuda'\n",
    "    ).chunk(2)\n",
    "    \n",
    "    sampled_images = autoencoder.decode(samples / opts.latent_scaling_factor) \n",
    "    sampled_images = torch.clamp(sampled_images, min=-1., max=1.)\n",
    "    sampled_images = unnormalize(sampled_images.permute(0, 2, 3, 1).cpu().numpy())\n",
    "    sampled_image = sampled_images[0]\n",
    "    video_frames.append(sampled_image)\n",
    "    \n",
    "    sam_predictor.set_image(sampled_image)\n",
    "    masks, _, _ = sam_predictor.predict(\n",
    "        point_coords=np.array([keypts_sequence[start_frame+k][0]]),\n",
    "        point_labels=np.array([1]),\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    hand_mask = masks[0]\n",
    "    mask = torch.tensor(\n",
    "            cv2.resize(masks[0].astype(int), dsize=opts.latent_size, interpolation=cv2.INTER_NEAREST), \n",
    "            dtype=torch.float, device='cuda').unsqueeze(0)[None, ...]\n",
    "    ref_cond = torch.cat([samples, target_cond[:, :-1], mask], 1) \n",
    "    if len(temp_ref_conds) >= last_N_frames and len(temp_ref_conds) > 0:\n",
    "        temp_ref_conds.pop(0)  \n",
    "    temp_ref_conds.append(torch.cat([samples, target_cond], 1))\n",
    "\n",
    "    #visualize\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(6*3, 6))\n",
    "    for i, vis_img in enumerate([ref_image, sampled_image]):\n",
    "        axs[i].imshow(vis_img)\n",
    "        axs[i].axis('off')\n",
    "        axs[i].grid(False)\n",
    "    visualize_hand(axs[2], keypts_sequence[start_frame+k], sampled_image)\n",
    "    axs[2].imshow(cv2.resize(\n",
    "        target_cond.cpu().numpy()[0, :42].sum(0), opts.image_size, interpolation=cv2.INTER_AREA), cmap='hot', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'{k}/{len(target_conds)}')\n",
    "    plt.show()\n",
    "    io.imsave(osp.join(save_dir, f'{idx}_{k}.jpg'), sampled_image)\n",
    "\n",
    "\n",
    "frames_to_video(video_frames, osp.join(save_dir, f'{idx}.mp4'), fps=20, rgb2bgr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handdiff_demo",
   "language": "python",
   "name": "handdiff_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
