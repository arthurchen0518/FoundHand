{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e45170a",
   "metadata": {},
   "source": [
    "# Load the diffusion model, SAM, Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19a27ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load diffusion model...\n",
      "[] ['projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias', 'projector.4.weight', 'projector.4.bias']\n",
      "Mediapipe hand detector and SAM ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750622747.767905  417560 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1750622747.800162  719404 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.129.03), renderer: NVIDIA RTX A6000/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1750622747.858383  718959 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1750622747.881670  718971 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import os.path as osp\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from models import vqvae \n",
    "from models import vit\n",
    "from diffusion import create_diffusion\n",
    "from utils.utils import (\n",
    "    scale_keypoint,  \n",
    "    keypoint_heatmap, \n",
    "    check_keypoints_validity)\n",
    "from utils.segment_hoi import init_sam, show_mask\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "\n",
    "def frames_to_video(frames, output_path, fps=30, resize=None, rgb2bgr=False):\n",
    "    \"\"\"\n",
    "    Convert a list of frames (numpy arrays) to a video using OpenCV.\n",
    "\n",
    "    Args:\n",
    "    - frames (list or numpy.ndarray): List of numpy frames.\n",
    "    - output_path (str): Path where the video will be saved.\n",
    "    - fps (int): Frames per second.\n",
    "    - resize (tuple or None): Resize frames to (width, height) if not None.\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(\"No frames provided\")\n",
    "\n",
    "    height, width = frames[0].shape[:2]\n",
    "\n",
    "    # If a resize shape is provided, adjust width and height\n",
    "    if resize is not None:\n",
    "        width, height = resize\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in tqdm(frames):\n",
    "        if len(frames[0].shape) == 2:\n",
    "            frame = frame[..., None].repeat(3, axis=-1)\n",
    "        if resize is not None:\n",
    "            frame = cv2.resize(frame, (width, height))\n",
    "        if rgb2bgr:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    \n",
    "\n",
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix) :]\n",
    "    return text\n",
    "\n",
    "\n",
    "def unnormalize(x):\n",
    "    return (((x + 1) / 2) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def make_ref_cond(img, keypts, hand_mask, device='cuda', target_size=(256, 256), latent_size=(32, 32)):\n",
    "    image_transform=Compose([\n",
    "        ToTensor(),\n",
    "        Resize(target_size),\n",
    "        Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], \n",
    "            std=[0.5, 0.5, 0.5], inplace=True),\n",
    "    ])\n",
    "    image = image_transform(img).to(device)\n",
    "    kpts_valid = check_keypoints_validity(keypts, target_size)\n",
    "    heatmaps = torch.tensor(keypoint_heatmap(\n",
    "        scale_keypoint(keypts, target_size, latent_size), \n",
    "        latent_size, var=1.) * kpts_valid[:, None, None], dtype=torch.float, device=device)[None, ...]\n",
    "    mask = torch.tensor(\n",
    "        cv2.resize(hand_mask.astype(int), dsize=latent_size, interpolation=cv2.INTER_NEAREST), \n",
    "        dtype=torch.float, device=device).unsqueeze(0)[None, ...]\n",
    "    return image[None, ...], heatmaps, mask\n",
    "\n",
    "\n",
    "def visualize_hand(ax, all_joints, img):\n",
    "# Define the connections between joints for drawing lines and their corresponding colors\n",
    "    connections = [\n",
    "        ((0, 1), 'red'), ((1, 2), 'green'), ((2, 3), 'blue'), ((3, 4), 'purple'),\n",
    "        ((0, 5), 'orange'), ((5, 6), 'pink'), ((6, 7), 'brown'), ((7, 8), 'cyan'),\n",
    "        ((0, 9), 'yellow'), ((9, 10), 'magenta'), ((10, 11), 'lime'), ((11, 12), 'indigo'),\n",
    "        ((0, 13), 'olive'), ((13, 14), 'teal'), ((14, 15), 'navy'), ((15, 16), 'gray'),\n",
    "        ((0, 17), 'lavender'), ((17, 18), 'silver'), ((18, 19), 'maroon'), ((19, 20), 'fuchsia')\n",
    "    ]\n",
    "    H, W, C = img.shape\n",
    "    \n",
    "    # Plot joints as points\n",
    "    ax.imshow(img)\n",
    "    for start_i in [0, 21]: \n",
    "        joints = all_joints[start_i: start_i+21]\n",
    "        for connection, color in connections:\n",
    "            joint1 = joints[connection[0]]\n",
    "            joint2 = joints[connection[1]]\n",
    "            ax.plot([joint1[0], joint2[0]], [joint1[1], joint2[1]], color=color)\n",
    "\n",
    "    ax.set_xlim([0, W])\n",
    "    ax.set_ylim([0, H])\n",
    "    ax.grid(False)\n",
    "    ax.set_axis_off()\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class HandDiffOpts:\n",
    "    run_name: str = 'ViT_256_handmask_heatmap_nvs_b25_lr1e-5'\n",
    "    sd_path: str = '/users/kchen157/scratch/weights/SD/sd-v1-4.ckpt'\n",
    "    log_dir: str = '/users/kchen157/scratch/log'\n",
    "    data_root: str = '/users/kchen157/data/users/kchen157/dataset/handdiff'\n",
    "    image_size: tuple = (256, 256)\n",
    "    latent_size: tuple = (32, 32)\n",
    "    latent_dim: int = 4\n",
    "    mask_bg: bool = False\n",
    "    kpts_form: str = 'heatmap'\n",
    "    n_keypoints: int = 42\n",
    "    n_mask: int = 1\n",
    "    noise_steps: int = 1000\n",
    "    test_sampling_steps: int = 250\n",
    "    ddim_steps: int = 100\n",
    "    ddim_discretize: str = \"uniform\"\n",
    "    ddim_eta: float = 0.\n",
    "    beta_start: float = 8.5e-4\n",
    "    beta_end: float = 0.012\n",
    "    latent_scaling_factor: float = 0.18215\n",
    "    cfg_pose: float = 5.\n",
    "    cfg_appearance: float = 3.5\n",
    "    batch_size: int = 25\n",
    "    lr: float = 1e-5\n",
    "    max_epochs: int = 500\n",
    "    log_every_n_steps: int = 100\n",
    "    limit_val_batches: int = 1\n",
    "    n_gpu: int = 8\n",
    "    num_nodes: int = 1\n",
    "    precision: str = '16-mixed'\n",
    "    profiler: str = 'simple'\n",
    "    swa_epoch_start: int = 10\n",
    "    swa_lrs: float = 1e-3\n",
    "    num_workers: int = 10\n",
    "    n_val_samples: int = 4\n",
    "        \n",
    "\n",
    "opts = HandDiffOpts()\n",
    "model_weights_dir = '../weights'\n",
    "model_path = osp.join(model_weights_dir, 'DINO_EMA_11M_b50_lr1e-5_epoch6_step320k.ckpt')\n",
    "vae_path = osp.join(model_weights_dir, 'vae-ft-mse-840000-ema-pruned.ckpt')\n",
    "sam_path = osp.join(model_weights_dir, 'sam_vit_h_4b8939.pth')\n",
    "\n",
    "print('Load diffusion model...')\n",
    "diffusion = create_diffusion(str(opts.test_sampling_steps))\n",
    "model = vit.DiT_XL_2(\n",
    "    input_size=opts.latent_size[0],\n",
    "    latent_dim=opts.latent_dim,\n",
    "    in_channels=opts.latent_dim+opts.n_keypoints+opts.n_mask,\n",
    "    learn_sigma=True,\n",
    ").cuda()\n",
    "\n",
    "# ckpt_state_dict = torch.load(model_path)['model_state_dict']\n",
    "ckpt_state_dict = torch.load(model_path)['ema_state_dict']\n",
    "missing_keys, extra_keys = model.load_state_dict(ckpt_state_dict, strict=False)\n",
    "model.eval()\n",
    "print(missing_keys, extra_keys)\n",
    "assert len(missing_keys) == 0\n",
    "\n",
    "\n",
    "vae_state_dict = torch.load(vae_path)['state_dict']\n",
    "autoencoder = vqvae.create_model(3, 3, opts.latent_dim).eval().requires_grad_(False).cuda()\n",
    "missing_keys, extra_keys = autoencoder.load_state_dict(vae_state_dict, strict=False)\n",
    "autoencoder.eval()\n",
    "assert len(missing_keys) == 0\n",
    "\n",
    "\n",
    "print('Mediapipe hand detector and SAM ready...')\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,  # Use False if image is part of a video stream\n",
    "    max_num_hands=2,         # Maximum number of hands to detect\n",
    "    min_detection_confidence=0.1)\n",
    "sam_predictor = init_sam(ckpt_path=sam_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c388f",
   "metadata": {},
   "source": [
    "# 3D functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd647a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_skeleton(ax, keypoints):\n",
    "    # Define connections between keypoints for OpenPose hand model\n",
    "    connections = [\n",
    "        (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "        (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "        (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "        (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "        (0, 17), (17, 18), (18, 19), (19, 20)\n",
    "    ]\n",
    "#     ax.scatter(keypoints[:, 0], keypoints[:, 2], keypoints[:, 1], c='red', marker='o', s=50)\n",
    "\n",
    "    # Plot connections\n",
    "    for connection in connections:\n",
    "        start, end = connection\n",
    "        ax.plot3D(keypoints[[start, end], 0], keypoints[[start, end], 1], keypoints[[start, end], 2], color='red')\n",
    "        \n",
    "    for connection in connections:\n",
    "        start, end = connection\n",
    "        start += 21\n",
    "        end += 21\n",
    "        ax.plot3D(keypoints[[start, end], 0], keypoints[[start, end], 1], keypoints[[start, end], 2], color='blue')\n",
    "        \n",
    "\n",
    "def area_of_convex_hull(points):\n",
    "    try:\n",
    "        return ConvexHull(points).volume\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def are_points_clustered_or_linear(points, reference_points, scale=0.5):\n",
    "    reference_area = area_of_convex_hull(reference_points)\n",
    "    points_area = area_of_convex_hull(points)\n",
    "    \n",
    "    # Check if the area formed by the points is less than the scaled area of the reference\n",
    "    return points_area < scale * reference_area\n",
    "\n",
    "\n",
    "def generate_intrinsic_matrix(size, focal):\n",
    "    cx = cy = size/2  # Center of the image\n",
    "    fx = fy = focal  # Focal length (arbitrary for this exercise)\n",
    "    intrinsic = np.array([[fx, 0, cx],\n",
    "                          [0, fy, cy],\n",
    "                          [0, 0, 1]])\n",
    "    return intrinsic\n",
    "\n",
    "\n",
    "def original_camera_extrinsic():\n",
    "    return np.eye(4)\n",
    "\n",
    "\n",
    "def unproject_points(points_3d, intrinsic):\n",
    "    # Separate the depth (z-values) from the (x, y) image coordinates\n",
    "    xy_image = points_3d[:, :2]\n",
    "    z_depth = points_3d[:, 2]\n",
    "\n",
    "    # Convert (x, y) to homogeneous coordinates\n",
    "    homogeneous_coords = np.hstack((xy_image, np.ones((xy_image.shape[0], 1))))\n",
    "\n",
    "    # Compute the inverse of the intrinsic matrix\n",
    "    K_inv = np.linalg.inv(intrinsic)\n",
    "\n",
    "    # Map the points to normalized camera coordinates\n",
    "    xy_normalized = np.dot(K_inv, homogeneous_coords.T).T  # (N, 3)\n",
    "\n",
    "    # Multiply the normalized coordinates by the z depth to get the 3D coordinates\n",
    "    points_camera_frame = -xy_normalized * z_depth[:, np.newaxis]\n",
    "\n",
    "    return points_camera_frame\n",
    "\n",
    "\n",
    "def compute_centroid(points_3d):\n",
    "    return np.mean(points_3d, axis=0)\n",
    "\n",
    "\n",
    "def get_rotated_camera_position(angle, centroid, original_camera_pos):\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), -np.sin(angle), 0],\n",
    "        [np.sin(angle), np.cos(angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    translated_camera = original_camera_pos - centroid\n",
    "    rotated_position = np.dot(rotation_matrix, translated_camera.T).T + centroid\n",
    "    return rotated_position\n",
    "\n",
    "\n",
    "def extrinsics_from_lookat(camera_pos, target_pos, up=np.array([0, 1, 0])):\n",
    "    z_axis = target_pos - camera_pos\n",
    "    z_axis = z_axis / np.linalg.norm(z_axis)\n",
    "\n",
    "    x_axis = np.cross(up, z_axis)  # Use the global up vector to define \"right\"\n",
    "    x_axis /= np.linalg.norm(x_axis)\n",
    "    \n",
    "    y_axis = np.cross(z_axis, x_axis)  # Re-compute the \"up\" vector\n",
    "    lookat_matrix = np.eye(4)\n",
    "    R = np.vstack((x_axis, y_axis, z_axis))\n",
    "    lookat_matrix[:3, :3] = R\n",
    "    lookat_matrix[:3, -1] = -R @ camera_pos\n",
    "    return lookat_matrix\n",
    "\n",
    "\n",
    "def project_points(points, K, E):\n",
    "    # Ensure points is a 2D array\n",
    "    points = np.asarray(points)\n",
    "    if points.ndim == 1:\n",
    "        points = points[np.newaxis, :]\n",
    "        \n",
    "    # Convert to homogeneous coordinates\n",
    "    if points.shape[1] == 3:\n",
    "        points_h = np.hstack([points, np.ones((points.shape[0], 1))])\n",
    "    else:\n",
    "        points_h = points\n",
    "    \n",
    "    # Transform points to camera coordinates\n",
    "    points_camera = E[:3] @ points_h.T  # 3xN matrix\n",
    "    \n",
    "    # Get depths (Z coordinates in camera space)\n",
    "    depths = points_camera[2]\n",
    "    \n",
    "    # Create mask for points in front of camera (positive Z)\n",
    "    mask = depths != 0\n",
    "    \n",
    "    # Avoid division by zero by setting invalid depths to 1\n",
    "    depths_safe = np.where(mask, depths, 1.0)\n",
    "    \n",
    "    # Project to image coordinates\n",
    "    points_proj = K @ points_camera\n",
    "    points_proj = points_proj / depths_safe\n",
    "    \n",
    "    # Convert to pixel coordinates (keep only x, y)\n",
    "    pixels = points_proj[:2].T\n",
    "    \n",
    "    return pixels\n",
    "\n",
    "\n",
    "def compute_azimuthal_angle(point, centroid):\n",
    "    delta = point - centroid\n",
    "    return np.arctan2(delta[2], delta[0])  # Based on x and z components\n",
    "\n",
    "\n",
    "def sample_upright_camera_positions_around_globe(centroid, distance, num_samples=5, height_factor=0.5):\n",
    "    sampled_positions = []\n",
    "    min_height, max_height = centroid[1] - height_factor * distance, centroid[1] + height_factor * distance\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        theta = np.random.uniform(0, 2*np.pi)  # Yaw\n",
    "        h = np.random.uniform(min_height, max_height)  # Height\n",
    "\n",
    "        # Compute the radius at this height for the spherical coordinates\n",
    "        r = np.sqrt(distance**2 - (h - centroid[1])**2)\n",
    "        \n",
    "        x = r * np.cos(theta) + centroid[0]\n",
    "        z = r * np.sin(theta) + centroid[2]\n",
    "\n",
    "        sampled_positions.append(np.array([x, h, z]))\n",
    "\n",
    "    return sampled_positions\n",
    "\n",
    "\n",
    "def get_direction_vector(camera_pos, centroid):\n",
    "    return camera_pos - centroid\n",
    "\n",
    "def rotation_matrix(axis, theta):\n",
    "    \"\"\"\n",
    "    Return the rotation matrix associated with counterclockwise rotation about\n",
    "    the given axis by theta radians.\n",
    "    \"\"\"\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis / np.sqrt(np.dot(axis, axis))\n",
    "    a = np.cos(theta / 2.0)\n",
    "    b, c, d = -axis * np.sin(theta / 2.0)\n",
    "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "    return np.array([\n",
    "        [aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "        [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "        [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]\n",
    "    ])\n",
    "\n",
    "\n",
    "def uniform_sphere_sampling(num_samples):\n",
    "    u = np.random.uniform(0, 1, num_samples)\n",
    "    v = np.random.uniform(0, 1, num_samples)\n",
    "    theta = 2 * np.pi * u  # Azimuthal angle\n",
    "    phi = np.arccos(2 * v - 1) - np.pi / 2  # Polar angle, subtracting np.pi/2 to map [-pi/2, pi/2]\n",
    "    return theta, phi\n",
    "\n",
    "def scale_keypoints(keypoints_3d):\n",
    "    # Compute the scaling factor: 1 divided by the maximum absolute coordinate value\n",
    "    scale_factor = 1.0 / np.max(np.abs(keypoints_3d))\n",
    "    \n",
    "    # Scale the 3D keypoints\n",
    "    scaled_keypoints = keypoints_3d * scale_factor\n",
    "\n",
    "    return scaled_keypoints\n",
    "\n",
    "def sample_sphere_vectors(N, radius):\n",
    "    phi = np.random.uniform(0, 2*np.pi, N)\n",
    "    cos_theta = np.random.uniform(-1, 1, N)\n",
    "    theta = np.arccos(cos_theta)\n",
    "    \n",
    "    # Convert to Cartesian coordinates\n",
    "    x = np.sin(theta) * np.cos(phi)\n",
    "    y = np.sin(theta) * np.sin(phi)\n",
    "    z = cos_theta\n",
    "    \n",
    "    # Stack the coordinates\n",
    "    vectors = np.stack([x, y, z], axis=-1) * radius\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def circular_camera_poses(radius, zenith_range, azimuth_range, num_poses=120):\n",
    "    camera_positions = []\n",
    "    zenith = np.linspace(zenith_range[0], zenith_range[1], num_poses)\n",
    "    azimuth = np.linspace(azimuth_range[0], azimuth_range[1], num_poses)\n",
    "    for i in range(num_poses):\n",
    "        # Convert spherical coordinates to Cartesian coordinates\n",
    "        x = radius * np.sin(np.radians(zenith[i])) * np.cos(np.radians(azimuth[i]))\n",
    "        z = radius * np.sin(np.radians(zenith[i])) * np.sin(np.radians(azimuth[i]))\n",
    "        y = radius * np.cos(np.radians(zenith[i])) \n",
    "        \n",
    "        camera_positions.append(np.array([x, y, z]))\n",
    "\n",
    "    return camera_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd7629-87f6-48cb-a7f9-9f307c22bc96",
   "metadata": {},
   "source": [
    "# Load reference views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dda994-a422-45d7-b98d-7059bb39d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "root = \n",
    "seq = '7-0032_pinkytip'\n",
    "data_root = './test_data/nvs_interhand'\n",
    "frame_cam_map = {}\n",
    "for pair in [file[:-4].split('-') for file in os.listdir(data_root)]:\n",
    "    if pair[0] not in frame_cam_map:\n",
    "        frame_cam_map[pair[0]] = set([pair[1]])\n",
    "    else:\n",
    "        frame_cam_map[pair[0]].add(pair[1])\n",
    "\n",
    "frame_id = '11929'\n",
    "ref_cam_id = '400002'\n",
    "\n",
    "N_test = 21\n",
    "test_cams = list(frame_cam_map[frame_id]) \n",
    "random.shuffle(test_cams)\n",
    "test_cams = test_cams[:N_test]\n",
    "test_cam_target_conds = []\n",
    "test_cam_keypts = []\n",
    "test_cam_gt = []\n",
    "test_cam_ids = []\n",
    "test_cam_params = {}\n",
    "for i, cam_id in enumerate(test_cams):\n",
    "    if cam_id == '400067':\n",
    "        continue\n",
    "        \n",
    "    img = io.imread(osp.join(data_root, f'{frame_id}-{cam_id}.jpg'))\n",
    "    original_img_size = tuple(img.shape[:2])\n",
    "    img = cv2.resize(img, opts.image_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    anns = np.load(osp.join(data_root, f'{frame_id}-{cam_id}.npz'))\n",
    "    hand_mask, keypts = anns['hand_mask'], anns['kpts']\n",
    "    hand_mask = np.array(hand_mask).astype(int)\n",
    "    keypts = scale_keypoint(keypts, original_img_size, opts.image_size)\n",
    "\n",
    "    masked_img = img * hand_mask[..., None] + 255*(1 - hand_mask[..., None])\n",
    "\n",
    "    image, heatmaps, mask = make_ref_cond(\n",
    "        img, keypts, hand_mask, device='cuda', target_size=opts.image_size, latent_size=opts.latent_size)\n",
    "    latent = opts.latent_scaling_factor * autoencoder.encode(image).sample()\n",
    "    \n",
    "    E = np.eye(4)\n",
    "    E[:3, :3] = anns['camrot']\n",
    "    E[:3, -1] = -anns['camrot'] @ anns['campos']\n",
    "    K = np.eye(3)\n",
    "    K[0, 0] = anns['focal'][0]\n",
    "    K[1, 1] = anns['focal'][1]\n",
    "    K[0, -1] = anns['princpt'][0]\n",
    "    K[1, -1] = anns['princpt'][1]\n",
    "    \n",
    "    test_cam_params[cam_id] = {\n",
    "        'joint3d': anns['joint3d'],\n",
    "        'joint_valid': anns['joint_valid'][..., None],\n",
    "        'K': K,\n",
    "        'E': E,\n",
    "    }\n",
    "    if cam_id == ref_cam_id:\n",
    "        ref_image = img\n",
    "        ref_keypts = keypts\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(3*2, 3))\n",
    "        visualize_hand(axs[0], keypts, img)\n",
    "        visualize_hand(axs[1], keypts, masked_img)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        src_ref_cond = torch.cat([latent, heatmaps, mask], 1) \n",
    "    else:\n",
    "        test_cam_ids.append(cam_id)\n",
    "        test_cam_keypts.append(keypts)\n",
    "        test_cam_gt.append(img)\n",
    "        test_cam_target_conds.append(torch.cat([heatmaps, torch.zeros_like(mask)], 1))\n",
    "        \n",
    "        \n",
    "save_dir = f'./test_data/nvs_test/{seq}'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "io.imsave(osp.join(save_dir, f'ref_{ref_cam_id}.jpg'), ref_image)\n",
    "with open(osp.join(save_dir, 'joint3d_cam_params.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_cam_params, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208cdfd",
   "metadata": {},
   "source": [
    "# Sample Test View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_scale = 3.0\n",
    "z = torch.randn((1, opts.latent_dim, opts.latent_size[0], opts.latent_size[1]), device='cuda')\n",
    "# novel view synthesis mode = on\n",
    "nvs = torch.ones(1, dtype=torch.int, device='cuda')\n",
    "z = torch.cat([z, z], 0)\n",
    "\n",
    "ref_conds = []\n",
    "for k, target_cond in enumerate(test_cam_target_conds[:3]):\n",
    "    model_kwargs = dict(\n",
    "        target_cond=torch.cat([target_cond, torch.zeros_like(target_cond)]), \n",
    "        ref_cond=torch.cat([src_ref_cond, torch.zeros_like(src_ref_cond)]), \n",
    "        nvs=torch.cat([nvs, 2 * torch.ones_like(nvs)]), \n",
    "        cfg_scale=cfg_scale)\n",
    "    \n",
    "    samples, _ = diffusion.p_sample_loop(\n",
    "        model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "        model_kwargs=model_kwargs, ref_conds=[src_ref_cond] + ref_conds, progress=True, device='cuda'\n",
    "    ).chunk(2)\n",
    "    ref_conds.append(torch.cat([samples, target_cond], 1))\n",
    "    \n",
    "    sampled_images = autoencoder.decode(samples / opts.latent_scaling_factor) \n",
    "    sampled_images = torch.clamp(sampled_images, min=-1., max=1.)\n",
    "    sampled_images = unnormalize(sampled_images.permute(0, 2, 3, 1).cpu().numpy())\n",
    "    sampled_image = sampled_images[0]\n",
    "\n",
    "    #visualize\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(6*4, 6))\n",
    "    for i, vis_img in enumerate([ref_image, test_cam_gt[k], sampled_image]):\n",
    "        axs[i].imshow(vis_img)\n",
    "        axs[i].axis('off')\n",
    "        axs[i].grid(False)\n",
    "        axs[i].set_title(test_cam_ids[k])\n",
    "    visualize_hand(axs[3], test_cam_keypts[k], sampled_image)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    io.imsave(osp.join(save_dir, f'gt_{test_cam_ids[k]}.jpg'), test_cam_gt[k])\n",
    "    io.imsave(osp.join(save_dir, f'sampled_{test_cam_ids[k]}.jpg'), sampled_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8f8d3",
   "metadata": {},
   "source": [
    "# Sample Camera Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_ref = test_cam_params[ref_cam_id]['E']\n",
    "hand_pose3d = (E_ref @ np.concatenate([test_cam_params[ref_cam_id]['joint3d'], np.ones((42, 1))], -1).transpose(1, 0)).transpose(1, 0)[:, :3]\n",
    "centroid = hand_pose3d[9].copy()\n",
    "hand_pose3d -= centroid\n",
    "E_ref_c2w = np.eye(4) \n",
    "E_ref_c2w[:3, -1] -= centroid\n",
    "E_ref_w2c = np.linalg.inv(E_ref_c2w)\n",
    "\n",
    "\n",
    "radius = (centroid ** 2).sum() ** 0.5\n",
    "zenith_range = (80, 100)\n",
    "azimuth_range = (-140, -40)\n",
    "num_poses = 45\n",
    "camera_positions = circular_camera_poses(radius, zenith_range, azimuth_range, num_poses=num_poses)\n",
    "cam_flythrough = np.array([extrinsics_from_lookat(camera_position, np.zeros(3)) for camera_position in camera_positions])\n",
    "np.savez(osp.join(save_dir, f'cam_motion.npz'), data=cam_flythrough)\n",
    "\n",
    "intrinsic = test_cam_params[ref_cam_id]['K']\n",
    "keypts_flythrough = []\n",
    "for i in range(num_poses):\n",
    "    projected_kpts = project_points(hand_pose3d, intrinsic, cam_flythrough[i])\n",
    "    keypts_flythrough.append(projected_kpts)\n",
    "\n",
    "\n",
    "target_conds = []\n",
    "for keypts in keypts_flythrough:\n",
    "    kpts_valid = check_keypoints_validity(keypts, opts.image_size)\n",
    "    target_heatmaps = torch.tensor(keypoint_heatmap(\n",
    "        scale_keypoint(keypts, opts.image_size, opts.latent_size), \n",
    "        opts.latent_size, var=1.) * kpts_valid[:, None, None], dtype=torch.float, device='cuda')[None, ...]\n",
    "    target_cond = torch.cat([\n",
    "        target_heatmaps, \n",
    "        torch.zeros_like(mask)], 1)\n",
    "    target_conds.append(target_cond)\n",
    "    \n",
    "\n",
    "cam_flythrough_c2w = np.linalg.inv(cam_flythrough)\n",
    "dirs =cam_flythrough_c2w[:, :3, :3] @ np.array([0, 0, 1])\n",
    "origins = cam_flythrough_c2w[:, :3, -1]\n",
    "\n",
    "ax = plt.figure(figsize=(12, 8)).add_subplot(projection='3d')\n",
    "plot_3d_skeleton(ax, hand_pose3d)\n",
    "\n",
    "_ = ax.quiver(\n",
    "  origins[..., 0].flatten(),\n",
    "  origins[..., 1].flatten(),\n",
    "  origins[..., 2].flatten(),\n",
    "  dirs[..., 0].flatten(),\n",
    "  dirs[..., 1].flatten(), \n",
    "  dirs[..., 2].flatten(),\n",
    "  length=0.1*radius, normalize=True, color='green')\n",
    "\n",
    "\n",
    "# original camera\n",
    "dirs = (E_ref_c2w[:3, :3] @ np.array([0, 0, 1]))[None]\n",
    "origins = E_ref_c2w[None, :3, -1]\n",
    "\n",
    "_ = ax.quiver(\n",
    "  origins[..., 0].flatten(),\n",
    "  origins[..., 1].flatten(),\n",
    "  origins[..., 2].flatten(),\n",
    "  dirs[..., 0].flatten(),\n",
    "  dirs[..., 1].flatten(), \n",
    "  dirs[..., 2].flatten(),\n",
    "  length=100, normalize=True, color='black')\n",
    "\n",
    "\n",
    "ax.set_xlim(-radius, radius)\n",
    "ax.set_ylim(-radius, radius)\n",
    "ax.set_zlim(-radius, radius)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "visualize_hand(fig.add_subplot(111), project_points(hand_pose3d, intrinsic, E_ref_w2c), ref_image)\n",
    "\n",
    "for kpts in keypts_flythrough:\n",
    "    fig = plt.figure()\n",
    "    visualize_hand(fig.add_subplot(111), kpts, ref_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa4025",
   "metadata": {},
   "source": [
    "# NVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48672105",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_N_frames = 1\n",
    "z = torch.randn((1, opts.latent_dim, opts.latent_size[0], opts.latent_size[1]), device='cuda')\n",
    "# novel view synthesis mode = on\n",
    "nvs = torch.ones(1, dtype=torch.int, device='cuda')\n",
    "z = torch.cat([z, z], 0)\n",
    "\n",
    "video_frames = [] \n",
    "last_frames = []\n",
    "for k, target_cond in enumerate(target_conds):\n",
    "    model_kwargs = dict(\n",
    "        target_cond=torch.cat([target_cond, torch.zeros_like(target_cond)]), \n",
    "        ref_cond=torch.cat([src_ref_cond, torch.zeros_like(src_ref_cond)]), \n",
    "        nvs=torch.cat([nvs, 2 * torch.ones_like(nvs)]), \n",
    "        cfg_scale=cfg_scale)\n",
    "    \n",
    "    samples, _ = diffusion.p_sample_loop(\n",
    "        model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "        model_kwargs=model_kwargs, ref_conds=[src_ref_cond] + ref_conds + last_frames, progress=True, device='cuda'\n",
    "    ).chunk(2)\n",
    "    \n",
    "    if len(last_frames) >= last_N_frames:\n",
    "        last_frames.pop(0)\n",
    "    last_frames.append(torch.cat([samples, target_cond], 1) )\n",
    "\n",
    "    \n",
    "    sampled_images = autoencoder.decode(samples / opts.latent_scaling_factor) \n",
    "    sampled_images = torch.clamp(sampled_images, min=-1., max=1.)\n",
    "    sampled_images = unnormalize(sampled_images.permute(0, 2, 3, 1).cpu().numpy())\n",
    "    sampled_image = sampled_images[0]\n",
    "    video_frames.append(sampled_image)\n",
    "\n",
    "    #visualize\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(6*3, 6))\n",
    "    for i, vis_img in enumerate([ref_image, sampled_image]):\n",
    "        axs[i].imshow(vis_img)\n",
    "        axs[i].axis('off')\n",
    "        axs[i].grid(False)\n",
    "    visualize_hand(axs[2], keypts_flythrough[k], sampled_image)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "frames_to_video(video_frames, osp.join(save_dir, 'cam_motion.mp4'), fps=15, rgb2bgr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bcc3b-56c3-41e8-b67a-7bf4711b282e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handdiff_demo",
   "language": "python",
   "name": "handdiff_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
